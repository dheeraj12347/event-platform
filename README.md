# ğŸ« Sydney Event Platform

A full-stack MERN-based event aggregation platform that automatically scrapes Sydney (Australia) events from multiple public sources, stores them in a database, displays them in a modern UI, and provides an admin dashboard with Google OAuth authentication.

---

## ğŸš€ Features

### ğŸ”„ Automated Event Scraping
- Scrapes events from multiple public event websites
- Stores:
  - Title
  - Date & Time
  - Venue Name & Address
  - City
  - Description
  - Category
  - Image URL
  - Source Website
  - Original Event URL
  - Last Scraped Time
- Automatically detects:
  - ğŸŸ¢ New events
  - ğŸŸ¡ Updated events
  - ğŸ”´ Inactive events (expired/removed)
- Scheduled scraping using `setInterval()`

---

### ğŸŒ Public Event Website
- Minimal, modern UI
- Event cards with:
  - Image
  - Title
  - Date/Time
  - Venue
  - Description
  - Source
  - Status Badge
  - **GET TICKETS CTA**

---

### ğŸ“© Email Capture Workflow
When user clicks **GET TICKETS**:
1. Modal asks for email
2. Consent checkbox required
3. Email + consent + event reference saved in DB
4. User redirected to original event URL

---

### ğŸ” Google OAuth Authentication
- Google Sign-In using OAuth 2.0
- JWT token-based authentication
- Only logged-in users can access dashboard

---

### ğŸ§‘â€ğŸ’¼ Admin Dashboard
Includes:

#### ğŸ” Filters
- City filter (default: Sydney, scalable)
- Keyword search (title / venue / description)
- Date range filter

#### ğŸ“Š Views
- Table view of events
- Clickable row â†’ preview panel

#### âš™ï¸ Actions
- Import to Platform button
- Stores:
  - importedAt
  - importedBy
  - status change

#### ğŸ· Status Tags
- `new`
- `updated`
- `inactive`
- `imported`

---

## ğŸ— Tech Stack

### Frontend
- React (Vite)
- Axios
- React Router
- Google OAuth
- Modern CSS (custom UI)

### Backend
- Node.js
- Express.js
- Sequelize ORM
- MySQL
- Axios + Cheerio (scraping)

---

## ğŸ“‚ Project Structure

```

event-platform/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ server.js
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ App.jsx
â”‚   â””â”€â”€ main.jsx

````

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/yourusername/event-platform.git
cd event-platform
````

---

### 2ï¸âƒ£ Backend Setup

```bash
cd backend
npm install
```

Create `.env` file:

```
PORT=5000
DB_NAME=event_platform
DB_USER=your_db_user
DB_PASSWORD=your_password
DB_HOST=localhost
JWT_SECRET=your_secret
```

Run backend:

```bash
node server.js
```

---

### 3ï¸âƒ£ Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

---

## ğŸ”„ Automatic Re-Scrape

The system runs scheduled scraping using:

```js
setInterval(() => {
  scrapeEvents();
}, 15 * 60 * 1000);
```

Detects:

* New events
* Updated events
* Inactive events

---

## ğŸ§ª Demonstrated Workflow

âœ” Scrape â†’ Store â†’ Display
âœ” Email Capture + Consent
âœ” OAuth Login
âœ” Protected Dashboard
âœ” Filters + Preview
âœ” Import â†’ Status Update
âœ” Scheduled Auto Re-Scrape

---

## ğŸ“Œ Future Improvements

* Real production scraping
* Pagination
* Role-based admin access
* Deployment (Docker / Cloud)

---

## ğŸ‘¨â€ğŸ’» Author

Your Name
Assignment Submission â€“ MERN Full Stack

```

---

# âœ… .gitignore (Root Level)

Create this at root of project:

```

# Node modules

node_modules/

# Environment files

.env
backend/.env
frontend/.env

# Logs

logs
*.log
npm-debug.log*

# OS files

.DS_Store
Thumbs.db

# Vite

frontend/node_modules/.vite/

# Build

dist/
build/

# Coverage

coverage/

# IDE

.vscode/
.idea/

# Mac

.DS_Store

# Windows

desktop.ini

````

---

# âš ï¸ Important Before Pushing to GitHub

### 1ï¸âƒ£ Make sure `.env` is NOT committed
Run:

```bash
git status
````

If `.env` shows â†’ remove:

```bash
git rm --cached backend/.env
```

---

### 2ï¸âƒ£ Make Clean Commits

```bash
git add .
git commit -m "Full MERN event platform with scraping, OAuth, dashboard"
git push origin main
```

---

# ğŸ† Submission Status

You now have:

âœ” Multi-source scraping
âœ” Auto update detection
âœ” Status tagging
âœ” Email capture
âœ” OAuth authentication
âœ” Protected dashboard
âœ” Import workflow
âœ” Scheduled automation
âœ” Modern UI

This is a **complete end-to-end MERN assignment**.

---